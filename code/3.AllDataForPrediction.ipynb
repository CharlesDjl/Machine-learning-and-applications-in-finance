{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset && DataLoader\n",
    "class HouseDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)  # Convert DataFrame to tensor\n",
    "        self.y = torch.tensor(y.values, dtype=torch.long)  # Convert pd.Series to numpy.ndarray and then to tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# PyTorch\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_prob=0.5, l2_reg=0.001):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "\n",
    "        self.l2_reg = l2_reg ## Adjust the value of l2_reg to find the suitable parameter\n",
    "        \n",
    "        self.best_loss = float('inf')\n",
    "        self.patience = 5\n",
    "        self.current_patience = 0\n",
    "\n",
    "    # Function: Relu(), using 0.15 dropout in every layer to reduce fit\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def l2_regularization(self):\n",
    "        l2_loss = 0\n",
    "        for param in self.parameters():\n",
    "            l2_loss += torch.norm(param, p=2) ** 2\n",
    "        return self.l2_reg * l2_loss\n",
    "    \n",
    "    def early_stop(self, val_loss):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.current_patience = 0\n",
    "        else:\n",
    "            self.current_patience += 1\n",
    "            if self.current_patience >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, train_loader, test_loader, num_epochs):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    ## If the pc has cuda, using it to calculate so that the speed is faster\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            # Move inputs and labels to the same device as the model\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels) + model.l2_regularization()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        scheduler.step() #Using the scheduler to adjust the learning rate every epoch\n",
    "        train_loss = running_loss / len(train_loader) # Calculate the training loss\n",
    "        losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(test_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction every fold\n",
    "def predict_fold(model, test_loader):\n",
    "    fold_true_labels = []\n",
    "    fold_predictions = []\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            fold_true_labels.extend(labels.cpu().numpy())\n",
    "            fold_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    return fold_true_labels, fold_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function, return accuracy, precision, recall and F1 score to evaluate the model\n",
    "def evaluate_fold(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='binary')\n",
    "    recall = recall_score(y_true, y_pred, average='binary')\n",
    "    f1 = f1_score(y_true, y_pred, average='binary')\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch [1/15], Loss: 0.5938, Val Loss: 0.2568\n",
      "Epoch [2/15], Loss: 0.4880, Val Loss: 0.2429\n",
      "Epoch [3/15], Loss: 0.4343, Val Loss: 0.2335\n",
      "Epoch [4/15], Loss: 0.4063, Val Loss: 0.2299\n",
      "Epoch [5/15], Loss: 0.3780, Val Loss: 0.2255\n",
      "Epoch [6/15], Loss: 0.3543, Val Loss: 0.2262\n",
      "Epoch [7/15], Loss: 0.3395, Val Loss: 0.2256\n",
      "Epoch [8/15], Loss: 0.3301, Val Loss: 0.2229\n",
      "Epoch [9/15], Loss: 0.3211, Val Loss: 0.2260\n",
      "Epoch [10/15], Loss: 0.3122, Val Loss: 0.2258\n",
      "Epoch [11/15], Loss: 0.3086, Val Loss: 0.2259\n",
      "Epoch [12/15], Loss: 0.3019, Val Loss: 0.2260\n",
      "Epoch [13/15], Loss: 0.2962, Val Loss: 0.2238\n",
      "Epoch [14/15], Loss: 0.2962, Val Loss: 0.2259\n",
      "Epoch [15/15], Loss: 0.2891, Val Loss: 0.2236\n",
      "Fold 1 Accuracy: 0.9112\n",
      "Fold 1 Precision: 0.7944\n",
      "Fold 1 Recall: 0.6043\n",
      "Fold 1 F1 Score: 0.6864\n",
      "Fold 1 Confusion Matrix:\n",
      "[[1649   51]\n",
      " [ 129  197]]\n",
      "Fold 2\n",
      "Epoch [1/15], Loss: 0.5732, Val Loss: 0.2489\n",
      "Epoch [2/15], Loss: 0.4789, Val Loss: 0.2353\n",
      "Epoch [3/15], Loss: 0.4347, Val Loss: 0.2333\n",
      "Epoch [4/15], Loss: 0.4008, Val Loss: 0.2192\n",
      "Epoch [5/15], Loss: 0.3771, Val Loss: 0.2257\n",
      "Epoch [6/15], Loss: 0.3580, Val Loss: 0.2203\n",
      "Epoch [7/15], Loss: 0.3358, Val Loss: 0.2188\n",
      "Epoch [8/15], Loss: 0.3234, Val Loss: 0.2142\n",
      "Epoch [9/15], Loss: 0.3169, Val Loss: 0.2159\n",
      "Epoch [10/15], Loss: 0.3131, Val Loss: 0.2149\n",
      "Epoch [11/15], Loss: 0.3048, Val Loss: 0.2142\n",
      "Epoch [12/15], Loss: 0.3015, Val Loss: 0.2164\n",
      "Epoch [13/15], Loss: 0.3000, Val Loss: 0.2143\n",
      "Epoch [14/15], Loss: 0.2933, Val Loss: 0.2171\n",
      "Epoch [15/15], Loss: 0.2899, Val Loss: 0.2190\n",
      "Fold 2 Accuracy: 0.9116\n",
      "Fold 2 Precision: 0.8075\n",
      "Fold 2 Recall: 0.5920\n",
      "Fold 2 F1 Score: 0.6832\n",
      "Fold 2 Confusion Matrix:\n",
      "[[1654   46]\n",
      " [ 133  193]]\n",
      "Fold 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 62\u001b[0m\n\u001b[0;32m     60\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m15\u001b[39m\n\u001b[0;32m     61\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 62\u001b[0m losses, val_losses \u001b[39m=\u001b[39m train_model(model, criterion, optimizer, scheduler, train_loader, test_loader, num_epochs)\n\u001b[0;32m     64\u001b[0m fold_true_labels, fold_predictions \u001b[39m=\u001b[39m predict_fold(model, test_loader)\n\u001b[0;32m     65\u001b[0m \u001b[39m# Get the evaluation rates every FOLD\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 16\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, scheduler, train_loader, test_loader, num_epochs)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m i, (inputs, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m     13\u001b[0m     \u001b[39m# Move inputs and labels to the same device as the model\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     inputs, labels \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 16\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     17\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, labels) \u001b[39m+\u001b[39m model\u001b[39m.\u001b[39ml2_regularization()\n\u001b[0;32m     19\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[3], line 38\u001b[0m, in \u001b[0;36mMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     36\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x)))\n\u001b[0;32m     37\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n\u001b[1;32m---> 38\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc2(x)))\n\u001b[0;32m     39\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout2(x)\n\u001b[0;32m     40\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc3(x)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[0;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[0;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[0;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    180\u001b[0m     bn_training,\n\u001b[0;32m    181\u001b[0m     exponential_average_factor,\n\u001b[0;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[0;32m    183\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[0;32m   2452\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the data from Bank1, Bank2, Bank3\n",
    "data1 = pd.read_csv('../data/Bank1_New.csv')\n",
    "data2 = pd.read_csv('../data/Bank2_New.csv')\n",
    "data3 = pd.read_csv('../data/Bank3_New.csv')\n",
    "\n",
    "# Combine them and reset the index of the data\n",
    "data = pd.concat([data1, data2, data3], ignore_index=True)\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.iloc[:, :-1]  # Features\n",
    "y = data.iloc[:, -1]  # Target variable\n",
    "\n",
    "# Normalize features (except the second to last column)\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = X.iloc[:, :-1]  # Exclude the second to last column (L-3)\n",
    "X_normalized = scaler.fit_transform(X_normalized)\n",
    "\n",
    "# Add the PCA result column back to the normalized features\n",
    "X_normalized = pd.DataFrame(X_normalized, columns=X.columns[:-1])\n",
    "X_normalized[\"L-3\"] = X.iloc[:, -2]  # Add the PCA result column\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model, loss function and optimizer\n",
    "input_size = X.shape[1]\n",
    "output_size = 2\n",
    "model = MLP(input_size, output_size)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=32, gamma=0.1)\n",
    "\n",
    "\n",
    "# Training the model\n",
    "num_splits = 5 # K = 5\n",
    "skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "\n",
    "metrics_per_fold = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    X_train, X_test = X_normalized.iloc[train_index], X_normalized.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    train_dataset = HouseDataset(X_train, y_train)\n",
    "    test_dataset = HouseDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = MLP(input_size, output_size)\n",
    "\n",
    "    # Cross-entropy is used to determine how close the actual output is to the desired output\n",
    "    # It is suitable for classification models\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=32, gamma=0.1)\n",
    "\n",
    "    num_epochs = 15\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    losses, val_losses = train_model(model, criterion, optimizer, scheduler, train_loader, test_loader, num_epochs)\n",
    "\n",
    "    fold_true_labels, fold_predictions = predict_fold(model, test_loader)\n",
    "    # Get the evaluation rates every FOLD\n",
    "    fold_accuracy, fold_precision, fold_recall, fold_f1 = evaluate_fold(fold_true_labels, fold_predictions)\n",
    "    print(f\"Fold {fold + 1} Accuracy: {fold_accuracy:.4f}\")\n",
    "    print(f\"Fold {fold + 1} Precision: {fold_precision:.4f}\")\n",
    "    print(f\"Fold {fold + 1} Recall: {fold_recall:.4f}\")\n",
    "    print(f\"Fold {fold + 1} F1 Score: {fold_f1:.4f}\")\n",
    "\n",
    "    metrics_per_fold.append((fold_accuracy, fold_precision, fold_recall, fold_f1))\n",
    "        \n",
    "    fold_confusion_matrix = confusion_matrix(fold_true_labels, fold_predictions)\n",
    "    print(f\"Fold {fold + 1} Confusion Matrix:\\n{fold_confusion_matrix}\")\n",
    "\n",
    "# Final results\n",
    "average_metrics = np.mean(metrics_per_fold, axis=0)\n",
    "print(f\"Average Accuracy: {average_metrics[0]:.4f}\")\n",
    "print(f\"Average Precision: {average_metrics[1]:.4f}\")\n",
    "print(f\"Average Recall: {average_metrics[2]:.4f}\")\n",
    "print(f\"Average F1 Score: {average_metrics[3]:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
